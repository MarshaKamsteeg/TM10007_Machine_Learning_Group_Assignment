{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tm10007_group_9_code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarshaKamsteeg/TM10007_Machine_Learning_Group_Assignment/blob/main/tm10007_group_9_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SXpaKwwGe5x"
      },
      "source": [
        "# TM10007 Assignment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "CiDn2Sk-VWqE"
      },
      "outputs": [],
      "source": [
        "# Run this to use from colab environment\n",
        "!pip install -q --upgrade git+https://github.com/karinvangarderen/tm10007_project.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghYv2pEkF8Eg"
      },
      "source": [
        "## Description of data set\n",
        "Import data from the ADNI dataset. Find empty values if present. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "-NE_fTbKGe5z",
        "outputId": "72ff1a89-8fe8-4167-d402-e153d934ad12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of samples: 855\n",
            "Total number of columns: 268\n",
            "Total amount of features: 266\n",
            "There are no empty values found.\n",
            "Within the dataset 519 samples are from people with Alzheimer Disease\n",
            "The other 336 samples are from people without Alzheimer Disease\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n"
          ]
        }
      ],
      "source": [
        "# Import Data\n",
        "from adni.load_data import load_data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Import Data\n",
        "data = load_data()\n",
        "data_pandas = pd.DataFrame(data)\n",
        "print(f'Total number of samples: {len(data.index)}')\n",
        "print(f'Total number of columns: {len(data.columns)}')\n",
        "print(f'Total amount of features: {len(data.columns)-2}')\n",
        "\n",
        "# Find empty / NaN entry in pandas dataframe\n",
        "data_pandas = data_pandas.replace(' ', np.nan) \n",
        "empty_values = np.where(pd.isnull(data_pandas))\n",
        "\n",
        "if ~empty_values[0] == []:\n",
        "  print(f'Empty values can be found at index {empty_values}')\n",
        "else:\n",
        "  print('There are no empty values found.')\n",
        "  pass\n",
        "\n",
        "# Percentage AD and CN\n",
        "number_AD = data_pandas['label'].value_counts()['AD']\n",
        "number_CN = data_pandas['label'].value_counts()['CN']\n",
        "print(f\"Within the dataset {number_AD} samples are from people with Alzheimer Disease\")\n",
        "print(f\"The other {number_CN} samples are from people without Alzheimer Disease\") \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing of the data"
      ],
      "metadata": {
        "id": "ZQTiKJ50EHlZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dividing data in a designset (80%) and testset (20%)"
      ],
      "metadata": {
        "id": "fTizaTtFMVOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import modules\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Divide data in label and feature set \n",
        "labels = data['label']\n",
        "features = data.drop(columns=['label'])\n",
        "\n",
        "# Divide data in trainingsset (80%) and testset (20%) stratified for the label\n",
        "data_designset, data_testset, label_designset, label_testset = train_test_split(features, labels, test_size=.2, random_state=1, stratify=data['label'])\n",
        "\n",
        "# Check the stratification \n",
        "percent_AD_train = (label_designset.value_counts()['AD'])/len(label_designset)*100\n",
        "percent_CN_train = (label_designset.value_counts()['CN'])/len(label_designset)*100\n",
        "percent_AD_test = (label_testset.value_counts()['AD'])/len(label_testset)*100\n",
        "percent_CN_test = (label_testset.value_counts()['CN'])/len(label_testset)*100\n",
        "\n",
        "print(f\"{percent_AD_train:.2f}% of the train data are Alzheimer disease samples.\")\n",
        "print(f\"{percent_CN_train:.2f}% of the train data are control samples.\")\n",
        "print(f\"{percent_AD_test:.2f}% of the test data are Alzheimer disease samples.\")\n",
        "print(f\"{percent_CN_test:.2f}% of the test data are control samples.\")\n"
      ],
      "metadata": {
        "id": "D-_lSnqzDCao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8b0e922-aedb-48e5-db28-103fdf8e89fd"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60.67% of the train data are Alzheimer disease samples.\n",
            "39.33% of the train data are control samples.\n",
            "60.82% of the test data are Alzheimer disease samples.\n",
            "39.18% of the test data are control samples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Dropping"
      ],
      "metadata": {
        "id": "5Cle-UrdMhyx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find columns where > 90% of the column has value zero. Remove the features if this is true. Besides, remove features if the whole column has one unique value. "
      ],
      "metadata": {
        "id": "EBORbJjgJM-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_designset = data_designset.drop(columns=data_designset.columns[data_designset.eq(0).mean()>0.9])\n",
        "data_designset = data_designset[data_designset.columns[data_designset.nunique() > 1]]\n",
        "data_designset_feature_names = data_designset.columns\n",
        "\n",
        "# Eventueel verwijderen als meer dan 90% hetzelfde is. \n",
        "# Toevoegen welke kolom die verwijdert."
      ],
      "metadata": {
        "id": "5vKxdt1OYemg"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scaling"
      ],
      "metadata": {
        "id": "bH8S0aUHMqlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Different methods for feature scaling. Decision will be made after usage of PCA\n",
        "# Import modules\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# # Robust scaler - based on percentiles. Not influenced by a few number of very large marginal outliers\n",
        "# trans = RobustScaler()\n",
        "# x_train_robust = trans.fit_transform(x_train)\n",
        "\n",
        "# # Standard scaler - removes the mean and scales the data to unit variance\n",
        "# scaler = StandardScaler()\n",
        "# x_train_scaler = scaler.fit_transform(x_train)\n",
        "\n",
        "# MinMax scaler - rescales data set such that all feature values are in range [0,1]\n",
        "norm = MinMaxScaler().fit(data_designset)\n",
        "data_designset_norm = norm.transform(data_designset)\n",
        "data_designset_norm = pd.DataFrame(data_designset_norm)\n",
        "data_designset_norm.columns = data_designset_feature_names"
      ],
      "metadata": {
        "id": "8s6_Mk4Yuq4V"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross validation\n",
        "First a stratified k fold cross validation is used with 10 splits. \n",
        "Feature selection - best method to select features from this multi feature space. The options looked in to are:\n",
        "1. Univariate\n",
        "2. Univariate + PCA\n",
        "3. PCA\n",
        "\n",
        "Classifier - best classifier with the remaining features. Three classifiers are looked in to based on literature review.\n",
        "1. K-NN (normalized scaler)\n",
        "2. SVM \n",
        "3. CNN"
      ],
      "metadata": {
        "id": "BIGLMKKeoMm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, StratifiedShuffleSplit\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import neighbors, metrics\n",
        "import seaborn\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "import math\n",
        "\n",
        "# configure the cross-validation procedure\n",
        "sss = StratifiedShuffleSplit(n_splits = 10, test_size = 0.1, random_state=1)\n",
        "k_list = list(range(1, 25, 2))\n",
        "all_train = []\n",
        "all_val = []\n",
        "\n",
        "for train_idx, val_idx in sss.split(data_designset_norm,label_designset): # split data\n",
        "  x_train, x_val = data_designset_norm.to_numpy()[train_idx], data_designset_norm.to_numpy()[val_idx]\n",
        "  y_train, y_val = label_designset.to_numpy()[train_idx], label_designset.to_numpy()[val_idx]\n",
        "\n",
        "  train_scores = []\n",
        "  validation_scores = []\n",
        "\n",
        "  bestfeatures = SelectKBest(score_func=f_classif, k=len(data_designset_norm.columns))\n",
        "  fit = bestfeatures.fit(x_train, y_train)\n",
        "  dfscores = pd.DataFrame(fit.scores_)\n",
        "  dfcolumns = pd.DataFrame(data_designset_norm.columns)\n",
        "\n",
        "  feature_scores = pd.concat([dfcolumns,dfscores],axis=1)\n",
        "  feature_scores.columns = ['Specs','Score']  \n",
        "  feature_scores = feature_scores.sort_values('Score')\n",
        "  highest_feature_score = feature_scores['Score'].iloc[-1]\n",
        "  threshold_feature_relative = 0.7                                                  # Optimization of parameter is needed\n",
        "  threshold_feature_score = threshold_feature_relative*highest_feature_score\n",
        "  criteria = feature_scores['Score']>=threshold_feature_score\n",
        "\n",
        "  feature_selection_names = []\n",
        "  for i in range(0,len(feature_scores['Score'])):\n",
        "    if feature_scores['Score'].iloc[i]>=threshold_feature_score:\n",
        "      feature_selection_names.append(feature_scores['Specs'].iloc[i])\n",
        "\n",
        "  selected_data_designset_norm = data_designset_norm[feature_selection_names]\n",
        "  print(selected_data_designset_norm)"
      ],
      "metadata": {
        "id": "qMg39eM5Ccq2",
        "outputId": "22a290ee-86e2-4a84-a5a6-50613f48dfd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     hf_quartile_range  tf_LBP_std_R3_P12  hf_kurtosis  hf_entropy\n",
            "0             0.198266           0.395872     0.150718    0.803665\n",
            "1             0.064497           0.763401     0.499410    0.314725\n",
            "2             0.038575           0.766152     0.494095    0.247117\n",
            "3             0.702242           0.330794     0.008194    0.837324\n",
            "4             0.049570           0.759462     0.620899    0.215376\n",
            "..                 ...                ...          ...         ...\n",
            "679           0.118290           0.588258     0.685016    0.357894\n",
            "680           0.039349           0.726369     0.525931    0.299269\n",
            "681           0.293117           0.479401     0.153013    0.612099\n",
            "682           0.076101           0.565875     0.527578    0.433078\n",
            "683           0.736996           0.420677     0.002345    0.862490\n",
            "\n",
            "[684 rows x 4 columns]\n",
            "       hf_std  hf_quartile_range  hf_kurtosis  hf_entropy\n",
            "0    0.281948           0.198266     0.150718    0.803665\n",
            "1    0.138342           0.064497     0.499410    0.314725\n",
            "2    0.101695           0.038575     0.494095    0.247117\n",
            "3    0.569036           0.702242     0.008194    0.837324\n",
            "4    0.086708           0.049570     0.620899    0.215376\n",
            "..        ...                ...          ...         ...\n",
            "679  0.351238           0.118290     0.685016    0.357894\n",
            "680  0.139765           0.039349     0.525931    0.299269\n",
            "681  0.400947           0.293117     0.153013    0.612099\n",
            "682  0.244049           0.076101     0.527578    0.433078\n",
            "683  0.582426           0.736996     0.002345    0.862490\n",
            "\n",
            "[684 rows x 4 columns]\n",
            "     hf_quartile_range  hf_kurtosis  hf_entropy\n",
            "0             0.198266     0.150718    0.803665\n",
            "1             0.064497     0.499410    0.314725\n",
            "2             0.038575     0.494095    0.247117\n",
            "3             0.702242     0.008194    0.837324\n",
            "4             0.049570     0.620899    0.215376\n",
            "..                 ...          ...         ...\n",
            "679           0.118290     0.685016    0.357894\n",
            "680           0.039349     0.525931    0.299269\n",
            "681           0.293117     0.153013    0.612099\n",
            "682           0.076101     0.527578    0.433078\n",
            "683           0.736996     0.002345    0.862490\n",
            "\n",
            "[684 rows x 3 columns]\n",
            "     hf_quartile_range  hf_kurtosis  hf_entropy\n",
            "0             0.198266     0.150718    0.803665\n",
            "1             0.064497     0.499410    0.314725\n",
            "2             0.038575     0.494095    0.247117\n",
            "3             0.702242     0.008194    0.837324\n",
            "4             0.049570     0.620899    0.215376\n",
            "..                 ...          ...         ...\n",
            "679           0.118290     0.685016    0.357894\n",
            "680           0.039349     0.525931    0.299269\n",
            "681           0.293117     0.153013    0.612099\n",
            "682           0.076101     0.527578    0.433078\n",
            "683           0.736996     0.002345    0.862490\n",
            "\n",
            "[684 rows x 3 columns]\n",
            "     hf_quartile_range  tf_LBP_std_R3_P12  hf_kurtosis  hf_entropy\n",
            "0             0.198266           0.395872     0.150718    0.803665\n",
            "1             0.064497           0.763401     0.499410    0.314725\n",
            "2             0.038575           0.766152     0.494095    0.247117\n",
            "3             0.702242           0.330794     0.008194    0.837324\n",
            "4             0.049570           0.759462     0.620899    0.215376\n",
            "..                 ...                ...          ...         ...\n",
            "679           0.118290           0.588258     0.685016    0.357894\n",
            "680           0.039349           0.726369     0.525931    0.299269\n",
            "681           0.293117           0.479401     0.153013    0.612099\n",
            "682           0.076101           0.565875     0.527578    0.433078\n",
            "683           0.736996           0.420677     0.002345    0.862490\n",
            "\n",
            "[684 rows x 4 columns]\n",
            "     tf_LBP_std_R3_P12  hf_quartile_range  hf_kurtosis  hf_entropy\n",
            "0             0.395872           0.198266     0.150718    0.803665\n",
            "1             0.763401           0.064497     0.499410    0.314725\n",
            "2             0.766152           0.038575     0.494095    0.247117\n",
            "3             0.330794           0.702242     0.008194    0.837324\n",
            "4             0.759462           0.049570     0.620899    0.215376\n",
            "..                 ...                ...          ...         ...\n",
            "679           0.588258           0.118290     0.685016    0.357894\n",
            "680           0.726369           0.039349     0.525931    0.299269\n",
            "681           0.479401           0.293117     0.153013    0.612099\n",
            "682           0.565875           0.076101     0.527578    0.433078\n",
            "683           0.420677           0.736996     0.002345    0.862490\n",
            "\n",
            "[684 rows x 4 columns]\n",
            "       hf_std  hf_quartile_range  hf_kurtosis  hf_entropy\n",
            "0    0.281948           0.198266     0.150718    0.803665\n",
            "1    0.138342           0.064497     0.499410    0.314725\n",
            "2    0.101695           0.038575     0.494095    0.247117\n",
            "3    0.569036           0.702242     0.008194    0.837324\n",
            "4    0.086708           0.049570     0.620899    0.215376\n",
            "..        ...                ...          ...         ...\n",
            "679  0.351238           0.118290     0.685016    0.357894\n",
            "680  0.139765           0.039349     0.525931    0.299269\n",
            "681  0.400947           0.293117     0.153013    0.612099\n",
            "682  0.244049           0.076101     0.527578    0.433078\n",
            "683  0.582426           0.736996     0.002345    0.862490\n",
            "\n",
            "[684 rows x 4 columns]\n",
            "     hf_quartile_range  tf_LBP_std_R3_P12  hf_kurtosis  hf_entropy\n",
            "0             0.198266           0.395872     0.150718    0.803665\n",
            "1             0.064497           0.763401     0.499410    0.314725\n",
            "2             0.038575           0.766152     0.494095    0.247117\n",
            "3             0.702242           0.330794     0.008194    0.837324\n",
            "4             0.049570           0.759462     0.620899    0.215376\n",
            "..                 ...                ...          ...         ...\n",
            "679           0.118290           0.588258     0.685016    0.357894\n",
            "680           0.039349           0.726369     0.525931    0.299269\n",
            "681           0.293117           0.479401     0.153013    0.612099\n",
            "682           0.076101           0.565875     0.527578    0.433078\n",
            "683           0.736996           0.420677     0.002345    0.862490\n",
            "\n",
            "[684 rows x 4 columns]\n",
            "     hf_kurtosis  hf_entropy\n",
            "0       0.150718    0.803665\n",
            "1       0.499410    0.314725\n",
            "2       0.494095    0.247117\n",
            "3       0.008194    0.837324\n",
            "4       0.620899    0.215376\n",
            "..           ...         ...\n",
            "679     0.685016    0.357894\n",
            "680     0.525931    0.299269\n",
            "681     0.153013    0.612099\n",
            "682     0.527578    0.433078\n",
            "683     0.002345    0.862490\n",
            "\n",
            "[684 rows x 2 columns]\n",
            "     hf_quartile_range  tf_LBP_std_R3_P12  hf_kurtosis  hf_entropy\n",
            "0             0.198266           0.395872     0.150718    0.803665\n",
            "1             0.064497           0.763401     0.499410    0.314725\n",
            "2             0.038575           0.766152     0.494095    0.247117\n",
            "3             0.702242           0.330794     0.008194    0.837324\n",
            "4             0.049570           0.759462     0.620899    0.215376\n",
            "..                 ...                ...          ...         ...\n",
            "679           0.118290           0.588258     0.685016    0.357894\n",
            "680           0.039349           0.726369     0.525931    0.299269\n",
            "681           0.293117           0.479401     0.153013    0.612099\n",
            "682           0.076101           0.565875     0.527578    0.433078\n",
            "683           0.736996           0.420677     0.002345    0.862490\n",
            "\n",
            "[684 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross Validation splitting of dataset"
      ],
      "metadata": {
        "id": "cKYfDtZQ5cNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, StratifiedShuffleSplit\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import neighbors, metrics\n",
        "import seaborn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# configure the cross-validation procedure\n",
        "sss = StratifiedShuffleSplit(n_splits = 10, test_size = 0.1, random_state=1)\n",
        "k_list = list(range(1, 25, 2))\n",
        "all_train = []\n",
        "all_val = []\n",
        "\n",
        "for train_idx, val_idx in sss.split(data_designset_norm,label_designset): # split data\n",
        "  x_train, x_val = data_designset_norm.to_numpy()[train_idx], data_designset_norm.to_numpy()[val_idx]\n",
        "  y_train, y_val = label_designset.to_numpy()[train_idx], label_designset.to_numpy()[val_idx]\n",
        "\n",
        "  train_scores = []\n",
        "  validation_scores = []\n",
        "\n",
        "# for k in k_list:\n",
        "#         clf_knn = neighbors.KNeighborsClassifier(n_neighbors=k)\n",
        "#         clf_knn.fit(X_train, y_train)\n",
        "\n",
        "#         # Test the classifier on the training data and plot\n",
        "#         score_train = clf_knn.score(X_train, y_train)\n",
        "#         score_val = clf_knn.score(X_val, y_val)\n",
        "    \n",
        "#         train_scores.append(score_train)\n",
        "#         validation_scores.append(score_val)\n",
        "    \n",
        "#     all_train.append(train_scores)\n",
        "#     all_val.append(validation_scores)\n",
        "    \n",
        "# # Create numpy array of scores and calculate the mean and std\n",
        "# all_train = np.array(all_train)\n",
        "# all_val = np.array(all_val)\n",
        "\n",
        "# train_scores_mean = all_train.mean(axis=0)\n",
        "# train_scores_std = all_train.std(axis=0)\n",
        "\n",
        "# val_scores_mean = all_val.mean(axis=0)\n",
        "# val_scores_std = all_val.std(axis=0)\n",
        "\n",
        "# # Plot the mean scores and the std as shading\n",
        "# fig = plt.figure(figsize=(8,8))\n",
        "# ax = fig.add_subplot(111)\n",
        "# ax.grid()\n",
        "# ax.fill_between(k_list, train_scores_mean - train_scores_std,\n",
        "#                      train_scores_mean + train_scores_std, alpha=0.1,\n",
        "#                      color=\"r\")\n",
        "# ax.fill_between(k_list, val_scores_mean - val_scores_std,\n",
        "#                      val_scores_mean + val_scores_std, alpha=0.1,\n",
        "#                      color=\"g\")\n",
        "# ax.plot(k_list, train_scores_mean, 'o-', color=\"r\",\n",
        "#         label=\"Training score\")\n",
        "# ax.plot(k_list, val_scores_mean, 'o-', color=\"g\",\n",
        "#         label=\"Validation score\")\n"
      ],
      "metadata": {
        "id": "hzGous2XmClP"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Univariate Feature Selection "
      ],
      "metadata": {
        "id": "22keSkOv408B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "import matplotlib.pyplot as plt\n",
        "import math \n",
        "\n",
        "bestfeatures = SelectKBest(score_func=f_classif, k=len(data_designset_norm.columns))\n",
        "fit = bestfeatures.fit(data_designset_norm, label_designset)\n",
        "dfscores = pd.DataFrame(fit.scores_)\n",
        "dfcolumns = pd.DataFrame(data_designset_norm.columns)\n",
        "\n",
        "feature_scores = pd.concat([dfcolumns,dfscores],axis=1)\n",
        "feature_scores.columns = ['Specs','Score']  \n",
        "feature_scores = feature_scores.sort_values('Score')\n",
        "highest_feature_score = feature_scores['Score'].iloc[-1]\n",
        "threshold_feature_relative = 0.7                                                  # Optimization of parameter is needed\n",
        "threshold_feature_score = threshold_feature_relative*highest_feature_score\n",
        "criteria = feature_scores['Score']>=threshold_feature_score\n",
        "\n",
        "feature_selection_names = []\n",
        "for i in range(0,len(feature_scores['Score'])):\n",
        "  if feature_scores['Score'].iloc[i]>=threshold_feature_score:\n",
        "    feature_selection_names.append(feature_scores['Specs'].iloc[i])\n",
        "\n",
        "selected_data_designset_norm = data_designset_norm[feature_selection_names]\n",
        "\n",
        "# print(feature_scores.nlargest(50,'Score'))  \n"
      ],
      "metadata": {
        "id": "aCGpuJGg46OH"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from sklearn import neighbors, metrics\n",
        "import seaborn\n",
        "\n",
        "# # Create a X fold stratified CV iterator\n",
        "# cv_outer = StratifiedKFold(n_splits=10)\n",
        "# results = []\n",
        "# best_n_neighbors = []\n",
        "\n",
        "# Loop over the folds\n",
        "for validation_index, test_index in cv_outer.split(features, labels):\n",
        "    \n",
        "\t\t# Split the data properly\n",
        "    data_train_val = features.to_numpy()[validation_index]\n",
        "    label_train_val = labels.to_numpy()[validation_index]\n",
        "   \n",
        "    data_test = features.to_numpy()[test_index]\n",
        "    label_test = labels.to_numpy()[test_index]\n",
        "\n",
        "    # Check the stratification \n",
        "    # percent_AD_train = (label_train_val == 'AD').sum()/len(label_train_val)*100\n",
        "    # percent_CN_train = (label_train_val == 'CN').sum()/len(label_train_val)*100\n",
        "    # percent_AD_test = (label_test == 'AD').sum()/len(label_test)*100\n",
        "    # percent_CN_test = (label_test == 'CN').sum()/len(label_test)*100\n",
        "    # print(f\"{percent_AD_train:.2f}% of the train data are Alzheimer disease samples.\")\n",
        "    # print(f\"{percent_CN_train:.2f}% of the train data are control samples.\")\n",
        "    # print(f\"{percent_AD_test:.2f}% of the test data are Alzheimer disease samples.\")\n",
        "    # print(f\"{percent_CN_test:.2f}% of the test data are control samples.\")\n",
        "    \n",
        "    # Create a grid search to find the optimal k using a gridsearch and 10-fold cross validation\n",
        "    # Same as above\n",
        "    parameters = {\"n_neighbors\": list(range(1, 10, 2))}\n",
        "    knn = neighbors.KNeighborsClassifier()\n",
        "    cv_inner = StratifiedKFold(n_splits=10)\n",
        "    grid_search = GridSearchCV(knn, parameters, cv=cv_inner.split(data_train_val, label_train_val), scoring='accuracy')\n",
        "    grid_search.fit(data_train_val, label_train_val)\n",
        "    \n",
        "    # Get resulting classifier\n",
        "    clf = grid_search.best_estimator_\n",
        "    print(f'Best classifier: k={clf.n_neighbors}')\n",
        "    best_n_neighbors.append(clf.n_neighbors)\n",
        "\n",
        "# \t\t# Test the classifier on the test data\n",
        "#     probabilities = clf.predict_proba(data_test)\n",
        "#     scores = probabilities[:, 1]\n",
        "    \n",
        "#     # Get the auc\n",
        "#     auc = metrics.roc_auc_score(label_test, scores)\n",
        "#     results.append({\n",
        "#         'auc': auc,\n",
        "#         'k': clf.n_neighbors,\n",
        "#         'set': 'test'\n",
        "#     })\n",
        "    \n",
        "    # Test the classifier on the validation data\n",
        "    probabilities_train_val = clf.predict_proba(data_train_val)\n",
        "    scores_train_val = probabilities_train_val[:, 1]\n",
        "    \n",
        "    # Get the auc\n",
        "    auc_validation = metrics.roc_auc_score(label_train_val, scores_train_val)\n",
        "    results.append({\n",
        "        'auc': auc_validation,\n",
        "        'k': clf.n_neighbors,\n",
        "        'set': 'validation'\n",
        "    })\n",
        "# Create results dataframe and plot it\n",
        "results = pd.DataFrame(results)\n",
        "seaborn.boxplot(y='auc', x='set', data=results)\n",
        "\n",
        "optimal_n = int(np.median(best_n_neighbors))\n",
        "print(f\"The optimal N={optimal_n}\")"
      ],
      "metadata": {
        "id": "AgRXmCdaox_F",
        "outputId": "3563182d-7f41-4988-f496-2757e5719569",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best classifier: k=15\n",
            "Best classifier: k=21\n",
            "Best classifier: k=11\n",
            "Best classifier: k=9\n",
            "Best classifier: k=13\n",
            "Best classifier: k=13\n",
            "Best classifier: k=23\n",
            "Best classifier: k=11\n",
            "Best classifier: k=13\n",
            "Best classifier: k=17\n",
            "The optimal N=13\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classifiers\n",
        "## PCA"
      ],
      "metadata": {
        "id": "0W9tsVcxvTKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# General Import\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Create a PCA which retains n principle components\n",
        "n_pca = 5\n",
        "pca = PCA(n_components=n_pca)\n",
        "\n",
        "# Fit the PCA model, point_data_train should be a 2D numpy array\n",
        "# which has one row per subject and the pointdata as columns where\n",
        "# the columns are x1, y1, z1, x2, y2, z2, ..., xn, yn, zn\n",
        "pca.fit(data_trainset)\n",
        "\n",
        "# Transform data\n",
        "data_trainset_trans = pca.transform(data_trainset)\n"
      ],
      "metadata": {
        "id": "C_vMuvALvpNs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}